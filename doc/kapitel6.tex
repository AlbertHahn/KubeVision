\chapter{Umsetzung des Lösungskonzepts}\label{Umsetzung}

Das folgende Kapitel beschreibt die Vorgehensweisen der Umsetzung des Konzepts.
Diese sind in mehrere Teile gegliedert.
Erstens die Konfiguration und Einrichtung der Knotenpunkte des Kubernetes-Cluster.
Zweitens die ausführende Entwicklung der Anwendung im Microservice-Architektur-Stil mit dem Namen KubeVision.
Drittens die Beschreibung der implementierten Gesichtserkennung des Facerecognition-Services.
Viertens die Dockerisierung der einzelnen Dienste.
Fünftens die Implementierung der Anwendung durch Helm-Charts für die Auslieferung und Bereistellung von Kubernetes-Ressourcenobjekte. 

\section{Konfiguration und Einrichtung}\label{konfig}

In diesem Abschnitt geht es um die Einrichtung und Vorkonfiguration des Kubernetes-Clusters mit der Distribution k3s.
Zuerst folgt die Einrichtung der einzelnen virtuellen privaten Server, die als Knotenpunkte in unserem Kubernetes-Cluster fungieren.
Danach die Konfiguration einer Domain für den späteren Einsatz der Microservices.
Abschließend erfolgt die Bereitstellung von Zertifikaten für die verschlüsselte Kommunikation über die Domain.

\subsubsection{Virtueller privater Server}

Durch die Einschränkungen, beschrieben in Abschnitt \ref{Einschraenkungen}, wird die Installation von Kubernetes auf \ac{vps}-Instanzen stattfinden.
Ein \acs{vps} wird von Drittanbietern als Internet-Hosting-Dienst angeboten und ermöglicht die Vermietung von Rechenleistung.
Die Server dienen als Knotenpunkte im Kubernetes-Cluster.
Es werden ingesamt drei VPS-Instanzen gemietet auf denen das Betriebssystem SLE-Micro Enterprise 5.1 bereitgestellt und auf den Serverinstanzen installiert.

\subsubsection{Domain}\label{Domain}
Der Zugang zur Webanwendung wird mithilfe einer öffentlichen Domain ermöglicht.
Der DNS-Eintrag einer Domain ist für die Adressierung zuständig.
Durch die Veränderung des A-Records werden alle Anfragen der Domain auf eine IPv4-Adresse umgeleitet \cite{LearningCoreDNS}.
Die IPv4-Adresse ist in diesem Fall die Server-Node des Kubernetes-Clusters.

\subsection{\acs{ssl}-Verschlüsselung}

Der Frontend-Service benötigt für die Gesichtserkennung die Webcam eines Benutzers, jedoch ist dies nur in einem sicheren Kontext möglich.
Die Kommunikation zwischen einem Client und Ingress muss \acs{tls}-Verschlüsselt sein, um JavaScript Methoden wie \textit{MediaDevices.getUserMedia()} auszuführen.
Dafür benötigt der Ingress-Controller ein Zertifikat und einen privaten Schlüssel.
Dieser kann automatischen mit einem Kubernetes-Issuer erstellt werden und von einem Ingress referenziert werden \cite{certmanager}.

\subsubsection{Issuer}
Das Add-On Cert-Manager ist bereits auf dem Kubernetes-Cluster vorinstalliert und ermöglicht die Verwaltung von Zertifikaten.
Cert-Manager enthält die Kubernetes-Resource Issuer, welche zur Generieriung von privaten Schlüsseln dient.
Cert-Manager erlaubt die vereinfachte Bereistellung von \ac{ssl}-Zertifikaten durch das \ac{acme} für Ingress-Objekte in Kubernetes.
Die \acs{acme}-Zertifikate sind frei verfügbar und werden von den meisten Webbrowsern als glaubwürdig eingestuft.
Die Verifizierung des Zertifikats erfolgt über eine \acs{acme}-Challenge, welche mit einer HTTP-Anfrage validiert werden kann.
Dafür wird ein berechneter Schlüssel auf dem Endpunkt der vorgegebenen Domain platziert und von einem öffentlichen \acs{acme}-Server abgerufen und bestätigt \cite{certmanageracme}.
Die Grundvoraussetzung dafür war die Änderung des A-Records auf die IPv4-Adresse der Server-Node in Abschnitt \ref{Domain}.
Die Ausführung des Issuers erzeugt einen privaten Schlüssel mit der Bezeichnung letsencrypt-key und dem Kubernetes-Issuer namens letsencrypt-prod. 
Für die Generieriung des Schlüssels wird die offene Zertifizierungsstelle Let's Encrypt verwendet \cite{letsencrypt} (vgl. Quellcode \ref{lst:issuer}). 

\begin{lstlisting}[caption={issuer.yaml \cite{certmanageracme} },captionpos=b,label={lst:issuer},language=yaml]
    apiVersion: cert-manager.io/v1
    kind: Issuer
    metadata:
      name: letsencrypt-prod
    spec:
      acme:
        server: https://acme-v02.api.letsencrypt.org/directory
        privateKeySecretRef:
          name: letsencrypt-key
        solvers:
        - http01:
           ingress:
             class: nginx

\end{lstlisting}



\subsubsection{Cert}
Der nächste Schritt ist die Erzeugung eines Zertifikats mit dem vorher erstellten Issuer.
Die Ausführung der Cert-YAML-Datei erstellt ein signiertes Zertifikat.
Dafür ist die Domain mit dem Eintrag der Server-Node und der Bezeichnung des Issuers notwendig.
Das erzeugte Secret mit der Bezeichnung deploy-secret kann von einem Ingress zur Verschlüsselung der Kommunikation verwendet werden (vgl. Quellcode \ref{lst:cert}). 

\begin{lstlisting}[caption={cert.yaml \cite{certmanageracme} },captionpos=b,label={lst:cert},language=yaml]
    apiVersion: cert-manager.io/v1
    kind: Certificate
    metadata:
      name: cert-prod
    spec:
      secretName: deploy-secret
      issuerRef: 
        name: letsencrypt-prod
      dnsNames:
      - "example-domain.com"

\end{lstlisting}


\subsection{Node-Affinity} \label{Nodeaffinity}
Für den Einsatz unterschiedlicher Hardwareressourcen in einem hybriden Kubernetes-Cluster muss eine Kennzeichnung der Nodes erfolgen.
Node-Affinity ermöglicht die Benutzung von Labels zur Zuweisung von spezifischen Werten.
Bei einer Auslieferung von Kubernetes-Anwendungen lassen sich diese dann auf bestimmte Nodes mit dem vorkonfigurierten Label bereitstellen \cite{nodeaffinity}.
In einem hybriden Kubernetes-Cluster kann somit die Unterteilung von Labels in Cloud- und On-Premise-Hardware erfolgen (vgl. Quellcode \ref{lst:labelnode}).


\begin{lstlisting}[caption={Node-Labels},captionpos=b,label={lst:labelnode},language=bash]
kubectl label nodes microservice0 hardware=cloud
kubectl label nodes microservice1 hardware=cloud
kubectl label nodes microservice2 hardware=premise
\end{lstlisting}

Die richtige Zuweisung von Pods auf gekennzeichneten Nodes erfolgt mit einem NodeSelector.
Diesem können Schlüsselwerte, wie die Kennzeichnung der Nodes übergeben werden, um Pods die Bereitstellung zu ermöglichen.

\subsection{Taints und Tolerations}
Taints und Tolerations stellt sicher, dass Pods nicht auf einen ungeeigneten Knoten eingeplant oder ausgeführt werden.
Ein Taint dient zur Markierung von Nodes.
Demnach akzeptieren diese nur Pods mit der richtigen Toleration.
Für die erforderliche Nutzung von \acs{gpu}-Nodes einer Anwendung können auch diese gekennzeichnet werden \cite{taintstolerations}.

\begin{lstlisting}[caption={Node-Taints},captionpos=b,label={lst:taints},language=bash]
kubectl taint nodes microservice2 hardware=gpu:NoSchedule
\end{lstlisting}

Damit werden nur Pods auf der Node \textit{microservice2} eingeplant, die als Toleration den Schlüssel \textit{hardware}, Wert \textit{gpu} und dem Effekt \textit{NoSchedule} (vgl. Quellcode \ref{lst:taints}) besitzt.
Bereits auf der Node laufende Pods sind davon nicht betroffen, dies erfordert den Taint \textit{NoExecute} \cite{taintstolerations}.

Die Ausführung von Node-Affinity und Taints und Tolerations ermöglicht nun die eindeutige Ausführung von Pods auf spezifischer Hardware.
Durch die Markierung mit Taint werden keine Pods ohne die Schlüsselwerte einer \acs{gpu} eingeplant.
Und die Bereitstellung von Pods lässt sich zielgerichtet auf die Nodes mit den spezifizierten Label bestimmen.

\section{Gesichtserkennung}
In diesem Abschnitt werden die zwei Algorithmen der OpenCV-Bibliothek erläutert, welche zur Gesichtserkennung im Facerecognition-Service in Einsatz kommen.

\subsection{Viola-Jones}

Die Viola-Jones Methode zur Erkennung von Objekten verwendet einen Klassifizierer.
Ein Klassifizierer verwendet Trainingsdaten, wie Datenpunkte, um den Zusammenhang von Eingabewerten im Kontext des Klassifizierers zu verstehen.
Für die Erkennung von Bildern mit Gesichtsmerkmalen wird dieser, mit positiven Bildern (mit Gesicht) und negativen Bildern (ohne Gesicht) trainiert.
Dabei werden die benötigten Gesichtsmerkmale aus den Bildern extrahiert.
Dafür wird ein Haar-Feature verwendet, das ähnlich wie ein Image-Kernel eine kleine Matrix darstellt und über die einzelnen Pixelwerte eines Bildes fährt und diese mit dem Kernel multipliziert.
Dadurch werden Merkmale, wie Augen, Mund und Nase, klassifiziert.
Für die schnelle Berechnung wird ein Cascade-Classifier verwendet. 
Bei der Klassifikation werden Bilder in Regionen aufgeteilt, die dann bei der Erkennung von Merkmalen weiter aufgeteilt werden. 
Wenn nicht, wird die Region übersprungen und die nächste berechnet, um Redundanz zu vermeiden.
OpenCV bietet hierfür bereits vortrainierte Modelle zur Erkennung von Gesichtsmerkmalen, die im Facerecognition-Service verwendet werden \cite{opencvcascade}.


\subsection{ \ac*{lbph} }
Für die Gesichtserkennung wird der Algorithmus \acs{lbph} verwendet.
Dieses fasst die lokalen Regionen in einem Bild zusammen, indem jeder Pixel mit Nachbarpixeln verglichen wird.
Dabei wird ein \ac{lbp}-Operator verwendet, das eine Region von einer 3x3-Matrix darstellt.
Das Pixel im Zentrum gilt als Schwellwert zur Berechnung der benachbarten Pixel.
Wenn der Wert eines Nachbarpixels gleich oder größer als der Wert des Pixels im Zentrum ist, wird die Pixelposition mit dem Wert 1 markiert.
Andernfalls wird der Pixel mit einer 0 markiert \cite{opencvlbp} (vgl. Abbildung \ref{fig:lbp}).

\begin{figure}[!htb]
  \centering
  \includegraphics[width=1.0\columnwidth]{images/lbp.png}
  \caption{Local Binary Patterns - Operator \cite{opencvlbp}}
  \label{fig:lbp}
\end{figure}

Für die Klassifizierung der räumlichen Information eines Bildes werden die lokalen Regionen als ein Histogram extrahiert, welches die Intensität der einzelnen Pixel ausgibt.
Dies führt dazu, dass Merkmale auch bei schlecht belichteten Bildern erkannt werden und weniger Fehlern bei der Gesichtserkennung mit dem \acs{lbph} stattfinden \cite{realtimefacerecog}.


\section{KubeVision}
Dieser Abschnitt behandelt die einzelnen Softwarekomponenten der Microservice-Anwendung KubeVision.
Die Webanwendung ist in drei verschiedene Dienste unterteilt.
Erstens einer Benutzeroberfläche für die Interaktion mit dem Benutzer.
Zweitens dem Authentifizierungsdienst, der für die Registrierung und Anmeldung zuständig ist.
Drittens einem Gesichtserkennungsdienst, welcher eine Zwei-Faktor-Authentifizierung per Gesichtserkennung ermöglicht.

\subsection{Frontend-Service}
Frontend-Service ist die Benutzeroberfläche zur Interaktion mit dem Benutzer.
Der Dienst ist aus mehreren Blueprints mit eigenen Endpunkten aufgebaut.
Jeder dieser Endpunkte gibt einen URL-Pfad für die Interaktion mit dem Frontend-Service oder einem anderen Dienst an.
Bei Aufruf eines Endpunkts wird eine View aufgerufen und mithilfe der Template Engine Jinja2 eine spezifische HTML-Datei aus dem templates-Verzeichnis ausgegeben.
Diese spezifische Datei ist ein HTML-Code-Block und wird in die Main-View gesetzt.

Es gibt zwei Blueprints. 
Der erste befindet sich im Verzeichnis home und stellt die Funktionalitäten und Authentication-Service-Endpunkte für das Anmelden, Registrieren und Anzeigen des Profils bereit (vgl. Abbildung \ref{fig:kubevision_home}).
Der zweite Blueprint besteht aus mit Views zur Interaktion mit dem Facerecognition-Service, welche erst nach einer erfolgreichen Anmeldung aufrufbar sind.
Der Benutzer wird mit seinem Namen begrüßt und erhält Anweisungen für die Erstellung von Bildern für den Facerecognition-Service.
Für die Nutzung des Facerecognition-Services wird die Kamera des Benutzers benötigt.
Die eingebundene JavaScript-Bibliothek SocketIO ermöglicht das Senden von Bildern, die von einer Webcam aufgenommen wurden.
Das Modul und die enthaltenen Funktionen in der Datei \textit{Camera.js} ist für die Verwendung der Webcam zuständig.
Die Funktion \textit{navigator.mediaDevices.enumerateDevices()} listet alle angeschlossenen Peripheregeräte mit Kamerafunktion auf.
Diese Geräte werden dann in eine Dropdown-Liste platziert.
Der Nutzer kann danach eine spezifische Kamera auswählen (vgl. Abbildung \ref{fig:kubevision_train}).

\begin{figure}[!htb]
  \centering
  \includegraphics[width=1.0\columnwidth]{images/kubevision_home.png}
  \caption{Frontend-Service - Home}
  \label{fig:kubevision_home}
\end{figure}



\begin{figure}[!htb]
  \centering
  \includegraphics[width=1.0\columnwidth]{images/Frontend-Service-train.png}
  \caption{Frontend-Service - Train}
  \label{fig:kubevision_train}
\end{figure}



Mit dem JavaScript-Modul \textit{socketio.js} lässt sich die bidirektionale Kommunikation mit dem Facerecognition-Service aufbauen.
Es gibt drei unterschiedliche Events für die Kommunikation mit dem Dienst.
Das Event \textit{stream} sendet 50 Bildern an den Dienst und löst im Anschluss ein Server-Event aus, das die Bilder des Benutzers auswertet.
Diese Interaktion ist über den Endpunkt \textit{/train} möglich.
Der Zweite Endpunkt \textit{/facelogin} ermöglicht die Kommunikation über das Event \textit{predict}.
Dieser sendet eine bestimmte Anzahl an Bildern an den Dienst und ermöglicht den Login des Nutzers.


\subsection{Authentication-Service}
Der Authentication-Service ist für die Authentifizierung des Benutzers über das Frontend zuständig.
Dieser Dienst wird mit einer Datenbank bereitgestellt in der Benutzerinformationen gespeichert werden.
Ein Blueprint stellt die API über dem Endpunkt \textit{/auth} bereit.
Die Routen des Endpunktes \textit{/auth} erhalten bis auf \textit{/auth/logout} ein POST-Objekt über den Frontend-Service.
In diesem steht der Name und das Passwort des Benutzers.
Damit kann sich ein Benutzer einen Account erstellen oder sich anmelden.
Der Dienst erstellt bei Registrierung einen Eintrag in die MongoDB-Datenbank oder liest diese aus.
Die Speicherung der Passwörter erfolgt in Form eines Hashes.
Bei erfolgreichem Login wird ein Cookie mit dem Benutzernamen gesetzt und der Benutzer wird in das Homemenü weitergeleitet (vgl. Abbildung \ref{fig:authenticationsequence}).

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.8\columnwidth]{images/UMLAuthenticationService.png}
  \caption{Ablauf von der Authentifizierung eines Benutzers}
  \label{fig:authenticationsequence}
\end{figure}

Hier kann der angemeldete Benutzer die Zwei-Faktor-Authentifizierung per Gesichtserkennung aktivieren und mit dem Facerecognition-Service kommunizieren.
Falls das Passwort falsch ist oder der Name bereits in der Datenbank eingetragen ist, wird der Benutzer mit einer Fehlermeldung benachrichtigt.
Bei der Abmeldung des Benutzers wird das Benutzer-Cookie gelöscht und der Benutzer wieder auf das Homemenü weitergeleitet.

\subsection{Facerecognition-Service}
Der Facerecognition-Service ermöglicht die Anmeldung eines Benutzers per Gesichtserkennung.
Grundvoraussetzung ist die Registrierung des Nutzers beim Authentication-Service.
Der Endpunkt wird komponentenbasiert über einen Blueprint realisiert.
Über den Endpunkt \textit{socketio} ist die eventbasierte Kommunikation zwischen Frontend-Service Benutzer und Facerecognition-Service möglich.
Das Event \textit{stream}, nimmt Bilder im webp-Format an und speichert diese in einem Verzeichnis.
Bei der Kommunikation wird nach jeder Anfrage ein Status zurückgeschickt.
Das Event \textit{traindata}, erstellt ein Klassenobjekt und führt die Funktion \textit{train()} aus.
Diese durchläuft das Bilderverzeichnis und erstellt ein Gesichtsdatenmodell zur späteren Validierung.
Das Event \textit{predict}, nimmt wie das Event \textit{stream} Bilder an, aber vergleicht diese mit dem vorher trainierten Modell für die Gesichtserkennung.
Bei erfolgreicher Übereinstimmung wird die Datenbank von Facerecognition-Service nach dem vorhandenen Benutzer überprüft (vgl. Quellcode \ref{fig:facerecognitioncomponents}).

\begin{figure}[!htb]
  \centering
  \includegraphics[width=1.0\columnwidth]{images/FaceRecognitionDiagramm.png}
  \caption{Komponentendiagramm der Facerecognition-Service Kommunikation }
  \label{fig:facerecognitioncomponents}
\end{figure}

Der eigentliche Entwurf sollte den Facerecognition-Service mit einer eigenen Datenbank ausliefern.
Aus Zeitgründen in der Entwicklung wurde dieser Teil verworfen und es wird die Datenbank des Authentication-Service genutzt.


\section{Dockerisierung}

Der nächste Schritt ist die Dockerisierung der losen Dienste.
Die Dienste liegen in einem eigenen Verzeichnis im Softwareprojekt KubeVision.

\subsection{Dockerfile}

Jeder Dienst verfügt über ein eigenes Dockerfile mit Anweisungen zum Erstellen eines Docker-Images.
Das Dockerfile befindet sich im selben Verzeichnis wie die Code-Dateien der Dienste.
Um Redundanz zu vermeiden, wird im Folgenden das Dockerfile zum Facerecognition-Service näher erläutert (vgl. Quellcode~\ref{lst:Dockerfile}).
Dieses ist ähnlich aufgebaut wie die Dockerfiles der anderen Dienste.
Die Basis des Docker-Images ist ein Python-Stretch-Image, welches auf dem leichtgewichtigen Betriebssystem Debian-Stretch aufbaut.
Zunächst werden die nötigen Abhängigkeiten zur Ausführung von OpenCV installiert.
Danach wird mit pip die notwendigen Pythonbibliotheken installiert.
In der Datei requirements.txt stehen alle Bibliothekennamen mit der erforderlichen Version.
Die umgebungsvariablen ermöglicht die Ausgabe des Python-Buffers im Terminal.
Die \textit{CMD}-Anweisung weist Docker an, den Containers immer mit dem Befehl, einen Gunicorn-Webserver auszuführen, zu starten.
Die zusätzlichen Flags geben die Art und Anzahl der Worker-Prozesse an.
Letztendlich wird die Webanwendung mit der WSGI-Schnittstelle an dem gewählten Port 5000 ausgeführt.

\begin{lstlisting}[caption={Dockerfile},captionpos=b ,label={lst:Dockerfile},language=Dockerfile]
FROM python:3.7.2-stretch

WORKDIR /app
ADD . /app

RUN apt-get update
RUN apt-get install ffmpeg libsm6 libxext6  -y
RUN pip install --upgrade pip setuptools wheel
RUN pip install -r requirements.txt

ENV PYTHONUNBUFFERED 1
EXPOSE 5000

CMD ["gunicorn" , "-k" ,"geventwebsocket.gunicorn.workers.GeventWebSocketWorker", "-w", "3" , "--bind" , ":5000" , "run:app"]
\end{lstlisting}


\subsection{Docker-Compose}

Der Build-Vorgang und der anschließende Ausführungsprozess mehrerer Dockerfiles kann mit dem Tool Docker-Compose vereinfacht werden.
Ähnlich zu Kubernetes-Ressourcenobjekten werden die Konfigurationen und Installationsansweisungen in einer YAML-Datei gespeichert.
Die Datei zur Ausführung von Docker-Compose liegt im Root-Verzeichnis des Projekts.

\begin{lstlisting}[caption={Ausschnit aus dem docker-compose.yaml},captionpos=b ,label={lst:docker-compose.yaml},language=yaml,float,floatplacement=H]
  authentication:
    build: ./authentication
    image: albird/authentication:latest
    environment:
      homeEndpoint: http://localhost:8000/
      trainEndpoint: http://localhost:8000/train 
      mongoEndpoint: mongodb://admin:password@localhost:27017/
    ports:
      - "5001:5001"
    command: gunicorn -w 1 -b 0.0.0.0:5001 run:app

  mongo:
    image: mongo:4.1.7
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password
    ports:
      - "27017:27017"
    volumes:
      - ./mongo-volume:/data/db
\end{lstlisting}

Der Befehl build gibt die Docker-Anweisung zum Bauen eines Images anhand einer vorhanden Dockerfile-Datei.
Die Webanwendung wurde mit dem Einsatz von umgebungsvariablen entwickelt.
Diese können für eine flexible Bereistellung der Dienste verwendet werden, um Endpunkte über String-Variablen in der Webanwendung zu ändern.
Ports gibt die Ports an, auf denen der erstellte Container im Netzwerk lauscht.
Die lokale Anwendung kann so im eigenen Hostnetzwerk erreicht werden.
Im Kubernetes-Cluster kann der Pod durch einen Service selektiert und mit einem Ingress verbunden werden.
Bei der MongoDB-Datenbank werden die umgebungsvariablen zur Übergabe von Passwort und Benutzernamen genutzt.
Der Volume Befehl erstellt ein persistentes Verzeichnis für die Speicherung der Datenbank.


\section{Helm-Chart}\label{helmcharts}
Dieser Abschnitt beschreibt die Entwicklung der Kubernetes-Ressourcenobjekte für die Bereitstellung mit dem Package-Manager Helm.
Helm-Charts verfügen über eine YAML-Datei namens Values, welche globale Parameter für das Helm-Chart definiert.
Dadurch können die Kubernetes-Ressourcenobjekte von einer Datei aus vorkonfiguriert werden.



\subsection{Service}
Dienste werden mit einem eigenen Kubernetes-Service bereitgestellt, um die Kommunikation zwischen Diensten und Kubernetes-Cluster zu realisieren.
Bis auf die individuellen Parameter des Helm-Charts, sind die Service-Konfiguration für alle Dienste identisch aufgebaut.

\begin{lstlisting}[caption={face-service.yaml},captionpos=b,label={lst:face-service},language=yaml,float,floatplacement=H]
  apiVersion: v1
  kind: Service
  metadata:
    name: {{ .Values.face.service.name }}
    namespace: {{ .Values.namespace }}
  spec:
    type: {{ .Values.face.service.type }}
    ports:
      - port: {{ .Values.face.image.port }}
        targetPort: {{ .Values.face.image.port }}
        protocol: TCP
        name: http
    selector:
      server: {{ .Values.face.name }}

\end{lstlisting}

Der Name des Kubernetes-Service ist für die spätere Angabe im Ingress notwendig.
\textit{Type} definiert den Service-Typ zur Kommunikation. 
Innerhalb des Clusters wird deshalb \textit{ClusterIP} gewählt.
Die \textit{Ports} geben an, welcher Port im lokalen Netzwerk des Pods lauscht.
\textit{Targetport} gibt dann den Port an, über den der Service erreichbar ist.
Schließlich wird der zugehörige Pod des Services mit dem \textit{Selector} ausgewählt.


\subsection{Ingress}

Für die Implementierung der Webanwendung wird ein Nginx-Ingress verwendet.
Dieser stellt den Endpunkt eines Services in Form einer URL dar.
Als Nächstes wird wie in Abschnitt \ref{nginx-ingress} beschrieben, ein Nginx-Ingress vorkonfiguriert (vgl. Quellcode \ref{lst:kubevisioningress}).


\begin{lstlisting}[caption={kubevision-ingress.yaml},captionpos=b,label={lst:kubevisioningress},language=yaml]
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
   name: kubevision-ingress
   annotations:
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
  spec:
    tls:
    - hosts:
      - {{ .Values.envEndpoint.host }}
      secretName: deploy-secret
    rules:
    - host: {{ .Values.envEndpoint.host }}
      http:
        paths:
        - backend:
            service:
              name: {{ .Values.auth.service.name }}
              port:
                number: {{ .Values.auth.image.port }}
          path: /auth
          pathType: Prefix
        - backend:
            service:
              name: {{ .Values.face.service.name }}
              port:
                number: {{ .Values.face.image.port }}
          path: /socket.io
          pathType: Prefix
        - backend:
            service:
              name: {{ .Values.frontend.service.name }}
              port:
                number: 80
          path: /
          pathType: Prefix
    ingressClassName: nginx

\end{lstlisting}

Der Bereich \textit{annotations} passt das Verhalten des Ingress an.
Die Optionen zum redirect mit SSL erzwingt die Weiterleitung von \acs{http} zu einer \acs{https} Verbindung mit dem Client.
Spec bestimmt die \acs{tls}-Verbindung und die Regeln für die Endpunkte der Services über den Ingress.
Als Hostname wird die Domain mit dem A-Record-Eintrag auf der Server-Node verwendet.
In der \acs{tls}-Einstellung wird noch das \acs{tls}-Zertifikat als Secret referenziert.
Jedem Service wird ein Endpunkt zugewiesen.
Der Authentication-Service ist über das Präfix \textit{auth} erreichbar.
Facerecognition-Service erhält den Endpunkt \textit{socket.io} zur Kommunkation mithilfe der gleichnamigen Bibliothek.
Der Frontend-Service ist über den Hostnamen erreichbar.
Für die Pfade wird kein Präfix wie bei dem Authentication-Service benötigt.



\subsection{Deployment}

Die grundlegende Bereitstellung der Dienste erfolgt mit einem Deployment.
Die Spezifikation des Deployments gibt die Anzahl der Replikationen der Pods an.
Der Selector ist für die Selektion von Pods durch das Deployment zuständig.
Das Template bezeichnet eine Menge von Pods mit einem Label.
Diese können dann von anderen Kubernetes-Objekten, wie Deployments und Services, selektiert werden.
Im \textit{spec} werden die Docker-Images zur Ausführung auf einem oder mehreren Pods angegeben.
Die \textit{imagePullPolicy} bestimmt die Regeln für das Herunterladen von Images.
Mit Always wird das gewählte Image immer heruntergeladen, auch wenn es sich bereits auf dem Hostssystem befindet.
\textit{Ports} gibt die lauschenden Ports des Containers im Pod an.
Die umgebungsvariablen definieren die Endpunkte des Dienstes.
\textit{NodeSelector} gibt die in Abschnitt \ref{Nodeaffinity} gekennzeichneten Nodes an, um zu bestimmen auf, welcher Hardware die Pods ausgeführt werden(vgl. Quellcode \ref{lst:deploymentauth}).


\begin{lstlisting}[caption={frontend-deployment.yaml },captionpos=b,label={lst:deploymentauth},language=yaml]
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: {{ .Values.frontend.name }}
    namespace: {{ .Values.namespace }}
  spec:
    replicas: {{ .Values.frontend.replicas }}
    selector:
      matchLabels:
        server: {{ .Values.frontend.name }}
    template:
      metadata:
        labels:
          server: {{ .Values.frontend.name }}
      spec:
        containers:
          - name: {{ .Values.frontend.name }}
            image: {{ .Values.frontend.image.name }}:{{ .Values.frontend.image.tag }}
            imagePullPolicy: Always
            ports:
              - containerPort: {{ .Values.frontend.image.port }}
            env:
            - name: loginEndpoint
              value: https://{{ .Values.envEndpoint.host }}/auth/login
            - name: registerEndpoint
              value: https://{{ .Values.envEndpoint.host }}/auth/register
            - name: websocketServer
              value: https://{{ .Values.envEndpoint.host }}
            - name: homeEndpoint
              value: https://{{ .Values.envEndpoint.host }}/
        nodeSelector:
          hardware: {{ .Values.frontend.nodeSelector.hardware }}

\end{lstlisting}



\subsection{PersistentVolumes}

Die persistente Speicherung von Benutzerinformationen oder Bildern erfolgt durch Persistent-Volumes.
Diese werden für die Dienste Authentication-Service und Facerecognition-Service benötigt.
Ein \ac{pv} ist ein Speicher, der in einem Kubernetes-Cluster von Administratoren oder dynamisch über Speicherklassen bereitgestellt wird.
Ein \ac{pvc} ist eine Anfrage zur Nutzung von \acs{pv}-Ressourcen in einem Kubernetes-Cluster.
Im Folgenden werden die Persistent-Volumes der MongoDB-Kubernetes-Ressourcenobjekte erläutert.


\subsubsection{PersistentVolumeClaim}

Ein \acs{pvc} benötigt Systemressourcen in Form von Festplattenspeicher.
Bei fehlenden \acs{pv} wird durch dynamische Provisionierung für die Anfrage ein \acs{pv} erstellt.

Die YAML-Datei für die MongoDB liegt in einem Unterverzeichnis mit den dazugehörigen Ressourcenobjekten.
Unter \textit{annotations} wird eine IF-Bedingung gestellt, die das Löschen nach der Nutzung des \acs{pvc} erlaubt. 
Wenn der Wert wahr ist, wird es nicht gelöscht.
Die \textit{annotations: }\textit{"helm.sh/resource-policy": keep} verhindert die Löschung einer Kubernetes-Ressource, wenn ein Helm-Chart deinstalliert wird \cite{helmtipsandtricks}.
Der \textit{spec} beschreibt die Speicherklassen des \acs{pvc}, dieser ist in dem Test-Cluster standardmäßig local-path.
Die \textit{accessModes} definieren die Zugriffsmodi des \acs{pv} wie beispielsweise die Lese- und Schreibrechte mehrerer Clients.
Für die MongoDB wird der Modus \textit{ReadWriteOnce} verwendet.
Dieser erlaubt Lese- und Schreibrechte für Pods, die sich auf derselben Node befinden.
\textit{Resources} definiert mit \textit{storage} die Menge an benötigten Speicherplatzes (vgl. Quellcode \ref{lst:pvc}).

\begin{lstlisting}[caption={mongodb-pvc.yaml },captionpos=b,label={lst:pvc},language=yaml]
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: {{ .Values.mongodbvolume.persistence.claimName }}
  annotations:
    {{- if .Values.skipuninstall }}
    "helm.sh/resource-policy": keep
    {{- end }}
spec:
  storageClassName: {{ .Values.mongodbvolume.persistence.storageClassName }}
  accessModes:
    {{- toYaml .Values.mongodbvolume.persistence.accessModes | nindent 4  }}
  resources:
    requests:
      storage: {{ .Values.mongodbvolume.persistence.storage }}
\end{lstlisting}

Der \acs{pvc} kann dann von dem MongoDB-Deployment referenziert werden 
und als Speicher in den Pod eingebunden werden. 
Das Standardverzeichnis für die Aufbewahrung von Daten in Mongodb ist \textit{/data/db} \cite{mongodbpath} (vgl. Quellcode \ref{lst:pv}).

\begin{lstlisting}[caption={Ausschnitt aus dem mongodb-deployment.yaml },captionpos=b,label={lst:pv},language=yaml]
  volumeMounts:
  - name: "mongo-data-dir"
    mountPath: "/data/db"
volumes:
- name: "mongo-data-dir"
  persistentVolumeClaim:
    claimName: "{{ .Values.mongodbvolume.persistence.claimName }}"
\end{lstlisting}
