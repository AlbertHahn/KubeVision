\chapter{Umsetzung des Lösungskonzepts}

Das folgende Kapitel beschreibt die Vorgehensweisen der Umsetzung des Konzepts.
Diese sind in mehrere Teile gegliedert.
Erstens mit der Konfiguration und Einrichtung der Knotenpunkte für das Kubernetes-Cluster.
Zweitens mit der ausführenden Entwicklung Anwendung im Microservice-Architektur-Stil KubeVision.
Drittens mit der Beschreibung für die implementierte Gesichtserkennung des Facerecognition-Services.
Viertens der Dockerisierung der einzelnen Dienste.
Fünftens die Ausführung der Helm-Charts für die Auslieferung und Bereistellung von Kubernetes-Ressourcenobjekte. 

\section{Konfiguration und Einrichtung}\label{konfig}

In diesem Abschnitt geht es um die Einrichtung der Kubernetes Infrastruktur.
Zuerst die Einrichtung der einzelnen virtuellen privaten Server in Vultr, die als Knotenpunkte in unserem Kubernetes-Cluster funktonieren.
Zunächst wird eine Domain für den späteren Einsatz der Microservices konfiguriert.
Abschließend erfolgt die Bereitstellung von Zertifikaten für die Domain.

\subsubsection{Virtueller privater Server}

Durch die Einschränkungen, beschrieben in Abschnitt \ref{Einschraenkungen},
werden für die Installation der Kubernetes Plattform virtuelle private Server (VPS) verwendet.
Ein VPS ist eine virtuelle Maschine, die von Drittanbietern wie Internet-Hosting-Diensten, als Dienst verkauft wird.
Dies ermöglicht das Mieten von Hardware.
Die Server dienen als Knotenpunkte für die spätere Kubernetes Installation.
Es werden ingesamt drei VPS-Instanzen gemietet auf denen das Betriebssystem SLE-Micro Enterprise 5.1 bereitgestellt und auf den Serverinstanzen installiert.

\subsubsection{Domain}\label{Domain}
Der Zugang zur Webanwendung wird mithilfe einer öffentlichen Domain ermöglicht.
Der DNS-Eintrag einer Domain ist für die Adressierung zuständig.
Durch die Veränderung des A-Records leiten alle Anfragen der Domain auf eine IPv4-Adresse um \cite{LearningCoreDNS}.
Die IPv4 Adresse ist in diesem Fall der Cluster-Master des Kubernetes-Clusters.

\subsection{SSL-Verschlüsselung}

Der Frontend-Service benötigt für die Gesichtserkennung die Webcam eines Benutzers, jedoch ist dies nur in einem sicheren Kontext möglich.
Die Kommunikation zwischen einem Client und Ingress muss TLS-Verschlüsselt sein, um JavaScript Methoden wie MediaDevices.getUserMedia() auszuführen.
Dafür benötigt der Ingress-Controller ein Zertifikat und einen privaten Schlüssel.
Dieser kann automatischen mit einem Kubernetes-Issuer erstellt werden und von einem Ingress referenziert werden \cite{certmanager}.

\textbf{Issuer}: 
Das add-on Cert-Manager ist bereits vorinstalliert auf dem Kubernetes-Cluster und ermöglicht die Verwaltung von Zertifikaten.
Dieser enthält die Kubernetes-Resource Issuer, welche zur Generieriung von privaten Schlüsseln dient.
Cert-Manager erlaubt die vereinfachte Bereistellung von \ac*{acme} SSL-Zertifikaten für Ingress-Objekte in Kubernetes.
Die ACME-Zertifikate sind frei verfügbar und werden von den meisten Webbrowsern als Glaubwürdig eingestuft.
Die Verifizierung des Zertifikats erfolgt über eine ACME-Challenge, welche mit einer HTTP-Anfrage validiert werden kann.
Dafür wird ein berechneter Schlüssel auf dem Endpunkt der vorgegebenen Domain platziert und von einem öffentlichen ACME-Server abgerufen und bestätigt \cite{certmanageracme}.
Die Grundvoraussetzung dafür war die Änderung des A-Records auf die IPv4-Adresse des Cluster-Masters in Abschnitt \ref{Domain}.


\begin{lstlisting}[caption={issuer.yaml \cite{certmanageracme} },captionpos=b,label={lst:issuer},language=yaml]
    apiVersion: cert-manager.io/v1
    kind: Issuer
    metadata:
      name: letsencrypt-prod
    spec:
      acme:
        server: https://acme-v02.api.letsencrypt.org/directory
        privateKeySecretRef:
          name: letsencrypt-key
        solvers:
        - http01:
           ingress:
             class: nginx

\end{lstlisting}

Die Ausführung des Issuers erzeugt einen privaten Schlüssel mit der Bezeichnung letsencrypt-key und dem Kubernetes-Issuer namens letsencrypt-prod.  


\textbf{Cert}: 
Der nächste Schritt ist die Erzeugung eines Zertifikats mit dem vorher erstellten Issuer.

\begin{lstlisting}[caption={cert.yaml \cite{certmanageracme} },captionpos=b,label={lst:cert},language=yaml]
    apiVersion: cert-manager.io/v1
    kind: Certificate
    metadata:
      name: cert-prod
    spec:
      secretName: deploy-secret
      issuerRef: 
        name: letsencrypt-prod
      dnsNames:
      - "example-domain.com"

\end{lstlisting}

Die Ausführung des Kubernetes-Cert erstellt ein signiertes Zertifikat.
Dafür ist die Domain mit dem Eintrag des Cluster-Masters notwendig und die Bezeichnung des Issuers.
Das erzeugte Secret mit der Bezeichnung deploy-secret kann von einem Ingress zur Verschlüsselung der Kommunikation verwendet werden.

\subsection{Node-Affinity} \label{Nodeaffinity}
Für den Einsatz unterschiedlicher Hardwareressourcen in einem hybriden Kubernetes-Cluster muss eine Kennzeichnung der Nodes erfolgen.
Node-Affinity ermöglicht die Benutzung von Labels zur Zuweisung von spezifischen Werten.
Bei einer Auslieferung von Kubernetes-Anwendungen lassen sich diese dann auf bestimmte Nodes mit dem vorkonfigurierten Label bereitstellen \cite{nodeaffinity}.
In einem hybriden Kubernetes-Cluster kann somit die Unterteilung von Labels in Cloud und On-Premise Hardware erfolgen (vgl. Quellcode \ref{lst:labelnode}).


\begin{lstlisting}[caption={Node-Labels},captionpos=b,label={lst:labelnode},language=bash]
kubectl label nodes microservice0 hardware=cloud
kubectl label nodes microservice1 hardware=cloud
kubectl label nodes microservice2 hardware=premise
\end{lstlisting}

Die richtige Zuweisung von Pods auf gekennzeichneten Nodes erfolgt mit einem NodeSelector.
Diesem können Schlüsselwerte wie die Kennzeichnung der Nodes übergeben werden, um Pods die Bereitstellung zu ermöglichen.

\subsection{Taints and Tolerations}
Taints und Tolerations stellt sicher, das Pods nicht auf einen ungeeigneten Knoten eingeplant werden.
Ein Taint dient zur Markierung von Nodes, demnach akzeptieren diese nur Pods mit der richtigen Toleration.
Für die erforderliche Nutzung von \acs{gpu}-Nodes einer Anwendung können auch diese gekennzeichnet werden \cite{taintstolerations}.

\begin{lstlisting}[caption={Node-Taints},captionpos=b,label={lst:taints},language=bash]
kubectl taint nodes microservice2 hardware=gpu:NoSchedule
\end{lstlisting}

Damit werden nur Pods auf der Node \textit{microservice2} eingeplant die als Toleration den Schlüssel Hardware, Wert gpu und dem Effekt NoSchedule (vgl. Quellcode \ref{lst:taints}).
Bereits auf der Node laufende Pods sind davon nicht betroffen, dies erfordert den Taint NoExecute \cite{taintstolerations}.

Die Ausführung von Node-Affinity und Taints und Tolerations ermöglicht nun die absolute Ausführung von Pods auf spezifischer Hardware.
Durch die Markierung mit Taint werden keine Pods ohne die Schlüsselwerte einer \acs{gpu} eingeplant.
Und die Bereitstellung von Pods lässt sich zielgerichtet auf die Nodes mit den spezifizierten Labels bestimmen.

\section{KubeVision}
Dieser Abschnitt behandelt die einzelnen Softwarekomponenten der Microservice-Anwendung KubeVision.
Die Webanwendung ist in drei verschiedene Dienste unterteilt.
Erstens der Benutzeroberfläche für die Interaktion mit dem Benutzer.
Zweitens dem Authentifizierungsdienst, der für die Registrierung und Anmeldung zuständig ist.
Drittens der Gesichtserkennungsdienst, welcher eine Zwei-Faktor-Authentifizierung per Gesichtserkennung ermöglicht.

\subsection{Frontend-Service}
Frontend-Service ist die Benutzeroberfläche zur Interaktion mit dem Benutzer.
Der Dienst ist in mehrere Blueprints mit eigenen Endpunkten aufgebaut.
Jeder dieser Endpunkte gibt einen URL-Pfad für die Interaktion mit dem Frontend-Service oder einem anderen Dienst an.
Bei Aufruf eines Endpunkts wird eine view aufgerufen und mithilfe der Template Engine Jinja2 eine spezifische HTML-Datei aus dem templates-Verzeichnis ausgegeben.
Diese spezifische Datei ist ein HTML Code-Block und wird in die Main-View gesetzt.

Es gibt Zwei Blueprints einer im Verzeichnis home, welcher die Funktionalitäten und Endpunkte für das einloggen, registrieren und anzeigen des Profils ausgibt.
Für die Authentifizierungsmöglichkeiten wird auf die Authentication-Service-Endpunkte umgeleitet.
Das Zweite Blueprint gibt views zum Interagieren mit dem Facerecognition-Service an.
Dieser beinhaltet Logik in Form von JavaScript mit der eingebundenen Bibliothek SocketIO und ermöglicht das Senden von Bildern mithilfe einer Webcam.

Für die Kommunikation mit dem Facerecognition-Service wird die Kamera des Benutzers benötigt.
Das Modul und die enthaltene Klasse Camera.js ist für die Verwendung der Webcam zuständig.
Die Funktion navigator.mediaDevices.enumerateDevices() listet alle angeschlossenen Peripheregeräte mit Kamerafunktion auf.
Diese Geräte werden dann in einer Schleife in eine Dropdown-Liste platziert.
Der Nutzer kann danach eine spezifische Kamera auswählen.

Mit der Klasse socketio.js lässt sich die bidirektionale Kommunikation mit dem Facerecognition-Service aufbauen.
Es gibt drei unterschiedliche Events für die Kommunikation mit dem Dienst.
Stream sendet eine bestimmte Anzahl an Bildern an den Dienst und löst im Anschluss das Event traindata aus.
Dieses Event wird über den Endpunkt train ermöglicht.
Der Zweite Endpunkt facelogin ermöglicht die Kommunikation über das Event Predict.
Dieser sendet eine bestimmte Anzahl an Bildern an den Dienst und ermöglicht den Login des Nutzers.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=1.0\columnwidth]{images/kubevision_home.png}
  \caption{KubeVision Frontend Dashboard}
  \label{fig:kubevision_home}
\end{figure}


\subsection{Authentication-Service}
Der Authentication-Service ist für die Authentifizierung des Benutzers über das Frontend zuständig.
Dieser Dienst wird mit einer Datenbank bereitgestellt in der Benutzerinformationen gespeichert werden.
Über ein Blueprint wird eine API mit dem Endpunkt auth bereitgestellt.
Damit kann sich ein Benutzer einen Benutzeraccount erstellen oder sich einloggen.
Der Dienst erstellt bei Registrierung einen Eintrag in die MongoDB-Datenbank oder lest diese aus.
Bei erfolgreichem Login wird ein Cookie mit dem Benutzernamen gesetzt und weitergeleitet in das Home-Hub.
Hier kann der Benutzer die Zwei-Faktor-Authentifizierung per Gesichtserkennung aktivieren und mit dem Facerecognition-Service kommunizieren.
Falls das Passwort falsch ist oder der Name bereits in der Datenbank eingetragen ist, wird der Benutzer mit einer Fehlermeldung benachrichtigt.
Bei der Abmeldung des Benutzers wird der Benutzer-Cookie gelöscht und wieder auf die Home-Page weitergeleitet.


\subsection{Facerecognition-Service}
Der Facerecognition-Service ermöglicht die Anmeldung eines Benutzers per Gesichtserkennung.
Grundvoraussetzung ist die Registrierung des Nutzers beim Authentication-Service.
Der Endpunkt wird Komponentenbasiert über eine Blueprint integriert.
Über den Endpunkt socketio ist die eventbasierte Kommunikation zwischen Frontend-Service Benutzer und Facerecognition-Service möglich.
Das Event stream, nimmt Bilder in Form von .webp an und speichert diese in einem Verzeichnis.
Bei der Kommunikation wird nach jeder Anfrage ein Status zurückgeschickt.
Das Event traindata, erstellt ein Klassenobjekt und führt die Funktion train() aus.
Diese geht durch das Bilderverzeichnis und erstellt ein Gesichtsdatenmodell zur späteren Validierung.
Das Event predict, nimmt wie das Event stream, Bilder an, aber vergleicht diese mit dem vorher trainierten Modell für die Gesichtserkennung.
Bei erfolgreicher Übereinstimmung wird die Datenbank von Facerecognition-Service nach dem vorhandenen Benutzer überprüft.
Der eigentliche Entwurf sollte den Facerecognition-Service mit einer eigenen Datenbank ausliefern.
Aus Zeitgründen in der Entwicklung wurde dieser Teil verworfen und es wird die Datenbank eines anderen Dienstes genutzt.


\section{Gesichtserkennung}
\subsection{Alignment}
\subsection{Training}
\subsection{Model}

\section{Dockerisierung}

Der nächste Schritt ist die Dockerisierung der losen Dienste.
Die Dienste liegen in einem eigenen Verzeichnis im Softwareprojekt KubeVision.

\subsection{Dockerfile}

Jeder Dienst verfügt über ein eigenes Dockerfile mit Anweisungen zum Erstellen eines Docker-Images.
Das Dockerfile befindet sich im selben Verzeichnis wie die Code-Dateien der Dienste.

\begin{lstlisting}[caption={Dockerfile},captionpos=b ,label={lst:Dockerfile},language=Dockerfile]
    FROM python:3.7.2-stretch
    
    WORKDIR /app
    ADD . /app
    
    RUN apt-get update
    RUN apt-get install ffmpeg libsm6 libxext6  -y
    RUN pip install --upgrade pip setuptools wheel
    RUN pip install -r requirements.txt
    
    ENV PYTHONUNBUFFERED 1
    EXPOSE 5000
    
    CMD ["gunicorn" , "-k" ,"geventwebsocket.gunicorn.workers.GeventWebSocketWorker", "-w", "3" , "--bind" , ":5000" , "run:app"]
    \end{lstlisting}

Um Redundanz zu vermeiden wird im Folgenden das Dockerfile zum Facerecognition-Service näher erläutert (vgl. Quellcode~\ref{lst:Dockerfile}).
Dieser ist ähnlich aufgebaut wie die Dockerfiles der anderen Dienste.
Die Basis des Docker-Images ist ein Python-Stretch-Image, welches auf dem leichtgewichtigen Betriebssystem Debian-Stretch aufbaut.
Zunächst werden die nötigen Abhängigkeiten zur Ausführung von OpenCV installiert.
Danach wird mit pip die notwendigen Pythonbibliotheken installiert.
In der requirements.txt stehen alle Bibliothekennamen mit der erforderlichen Version.
Die Enviornmental-Variable ermöglicht die Ausgabe des Python-Buffers im Terminal.
Die CMD Anweisung des Containers startet immer mit dem Befehl einen Gunicorn-Webserver auszuführen.
Die zusätzlichen Flags geben die Art und Anzahl der Worker-Prozesse an.
Letztendlich wird die Webanwendung mit der WSGI-Schnittstelle an den gewählten Port 5000 ausgeführt.

\subsection{Docker-Compose}

Der Build-Vorgang und anschließende Ausführungsprozess mehrerer Dockerfiles kann mit dem Tool Docker-Compose vereinfacht werden.
Ähnlich wie bei Kubernetes-Ressourcenobjekte werden die Konfigurationen und Installationsansweisungen in einer YAML-Datei gespeichert.
Die Datei zur Ausführung von Docker-Compose liegt im Root-Verzeichnis des Projekts.

\begin{lstlisting}[caption={Ausschnit aus dem docker-compose.yaml},captionpos=b ,label={lst:docker-compose.yaml},language=Dockerfile]
  authentication:
    build: ./authentication
    image: albird/authentication:latest
    environment:
      homeEndpoint: http://localhost:8000/
      trainEndpoint: http://localhost:8000/train 
      mongoEndpoint: mongodb://admin:password@localhost:27017/
    ports:
      - "5001:5001"
    command: gunicorn -w 1 -b 0.0.0.0:5001 run:app

  mongo:
    image: mongo:4.1.7
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password
    ports:
      - "27017:27017"
    volumes:
      - ./mongo-volume:/data/db
  \end{lstlisting}

Der Befehl build gibt die Docker-Anweisung zum Bauen eines Images anhand einer vorhanden Dockerfile-Datei.
Die Webanwendung wurde mit dem Einsatz von Enviornmental-Varibales entwickelt.
Diese können für eine flexiblen Bereistellung der Dienste angewendet werden, um Endpunkte in Form von String-Variablen in der Webanwendung zu ändern.
Ports gibt die Ports an auf die der erstellte Container im Netzwerk lauscht.
Die lokale Anwendung kann so im eigenen Hostnetzwerk erreicht werden.
Im Kubernetes-Cluster kann der Pod durch einen Service selektiert und mit einem Ingress verbunden werden.
Bei der MongoDB-Datenbank werden die Enviornmental-Variables zur Übergabe von Passwort und Adminnamen genutzt.
Der Volume Befehl erstellt ein persistentes Verzeichnis für die Speicherung der Datenbank.


\section{Helm-Chart}
Dieser Abschnitt beschreibt die Entwicklung der Kubernetes-Ressourcenobjekte für die Bereitstellung mit dem Package-Manager Helm.
Um Redundanz zu vermeiden wird die Umsetzung der Kubernetes-Ressourcenobjekte beispielhaft am Dienst Facerecognition gezeigt.
Helm-Charts verfügen über eine YAML-Datei namens Values, welche globale Parameter für das Helm-Chart definiert.
Dadurch können die Kubernetes-Ressourcenobjekte von einer Datei aus vorkonfiguriert werden.



\subsection{Service}
Dienste werden mit einem eigenen Kubernetes-Service bereitgestellt, um die Kommunikation zwischen Diensten und Kubernetes-Cluster zu realisieren.
Bis auf die individuellen Parameter des Helm-Charts, sind die Service-Konfiguration für alle Dienste identisch aufgebaut.

\begin{lstlisting}[caption={face-service.yaml},captionpos=b,label={lst:face-service},language=yaml]
  apiVersion: v1
  kind: Service
  metadata:
    name: {{ .Values.face.service.name }}
    namespace: {{ .Values.namespace }}
  spec:
    type: {{ .Values.face.service.type }}
    ports:
      - port: {{ .Values.face.image.port }}
        targetPort: {{ .Values.face.image.port }}
        protocol: TCP
        name: http
    selector:
      server: {{ .Values.face.name }}

\end{lstlisting}

Der Name des Kubernetes-Service ist für die spätere Angabe im Ingress notwendig.
Type definiert den Service-Typen zur Kommunikation innerhalb des Clusters wird deshalb ClusterIP gewählt.
Die Ports geben an, welcher Port im lokalen Netzwerk des Pods lauscht.
TargetPort gibt dann den Port an der über den Service erreichbar ist.
Schließlich wird der zugehörige Pod des Services mit dem Selector ausgewählt.


\subsection{Ingress}

Für die Implementierung der Webanwendung wird ein Nginx-Ingress verwendet.
Dieser stellt den Endpunkt eines Services in Form einer URL dar.
Als Nächstes wird wie in Abschnitt \ref{nginx-ingress} beschrieben ein Nginx-Ingress vorkonfiguriert (vgl. Quellcode \ref{lst:kubevisioningress}).


\begin{lstlisting}[caption={kubevision-ingress.yaml},captionpos=b,label={lst:kubevisioningress},language=yaml]
  apiVersion: networking.k8s.io/v1
  kind: Ingress
  metadata:
   name: kubevision-ingress
   annotations:
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
  spec:
    tls:
    - hosts:
      - {{ .Values.envEndpoint.host }}
      secretName: deploy-secret
    rules:
    - host: {{ .Values.envEndpoint.host }}
      http:
        paths:
        - backend:
            service:
              name: {{ .Values.auth.service.name }}
              port:
                number: {{ .Values.auth.image.port }}
          path: /auth
          pathType: Prefix
        - backend:
            service:
              name: {{ .Values.face.service.name }}
              port:
                number: {{ .Values.face.image.port }}
          path: /socket.io
          pathType: Prefix
        - backend:
            service:
              name: {{ .Values.frontend.service.name }}
              port:
                number: 80
          path: /
          pathType: Prefix
    ingressClassName: nginx

\end{lstlisting}

Der Bereich annotations passt das Verhalten des Ingress an.
Die Optionen zum redirect mit SSL erzwingt die Weiterleitung von HTTP zu einer HTTPS Verbindung mit dem Client.
Spec bestimmt die TLS-Verbindung und die Regeln für die Endpunkte der Services über den Ingress.
Als Hostname wird die Domain mit dem A-Record Eintrag auf dem Cluster-Master verwendet.
In der TLS-Einstellung wird noch das TLS-Zertifikat als Secret referenziert.
Jedem Service wird ein Endpunkt zugewiesen.
Der Authentication-Service ist über den Prefix auth erreichbar.
Facerecognition-Service erhält den Endpunkt socket.io zur Kommunkation mithilfe der gleichnamigen Bibliothek.
Der Frontend-Service ist über den Hostnamen erreichbar.
Für die Pfade wird kein Prefix wie bei dem Authentication-Service benötigt.



\subsection{Deployment}

Die grundlegende Bereitstellung der Dienste erfolgt mit einem Deployment.

\begin{lstlisting}[caption={deployment.yaml },captionpos=b,label={lst:deploymentauth},language=yaml]
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: {{ .Values.frontend.name }}
    namespace: {{ .Values.namespace }}
  spec:
    replicas: {{ .Values.frontend.replicas }}
    selector:
      matchLabels:
        server: {{ .Values.frontend.name }}
    template:
      metadata:
        labels:
          server: {{ .Values.frontend.name }}
      spec:
        containers:
          - name: {{ .Values.frontend.name }}
            image: {{ .Values.frontend.image.name }}:{{ .Values.frontend.image.tag }}
            imagePullPolicy: Always
            ports:
              - containerPort: {{ .Values.frontend.image.port }}
            env:
            - name: loginEndpoint
              value: https://{{ .Values.envEndpoint.host }}/auth/login
            - name: registerEndpoint
              value: https://{{ .Values.envEndpoint.host }}/auth/register
            - name: websocketServer
              value: https://{{ .Values.envEndpoint.host }}
            - name: homeEndpoint
              value: https://{{ .Values.envEndpoint.host }}/
        nodeSelector:
          hardware: {{ .Values.frontend.nodeSelector.hardware }}

\end{lstlisting}

Die Spezifikation des Deployments gibt die Anzahl der Replikationen der Pods an.
Der Selector ist für die Selektion von Pods durch das Deployment zuständig.
Das Template bezeichnet eine Menge von Pods mit einem Label, diese können dann von anderen Kubernetes-Objekten, wie Deployments und Services selektiert werden.
Im \textit{spec} werden die Docker-Images angegeben zur Ausführung auf einem oder mehreren Pods.
Die \textit{imagePullPolicy} bestimmt die Regeln für das Herunterladen von Images.
Mit Always wird das gewählte Image immer heruntergeladen auch, wenn es sich bereits auf dem Hostssystem befindet.
Ports gibt die lauschenden Ports des Containers im Pod an.
Die Enviornmental-Variables sind für die Definition der Endpunkte des Dienstes.
\textit{NodeSelector} gibt die in Abschnitt \ref{Nodeaffinity} gekennzeichneten Nodes an, um zu bestimmen auf, welcher Hardware die Pods ausgeführt werden.


\subsection{Persistent-Volumes}

Die persistente Speicherung von Benutzerinformationen oder Bildern erfolgt durch Persistent-Volumes.
Diese werden für die Dienste Authentication-Service und Facerecognition-Service benötigt.
Ein \ac{pv} ist ein Speicher der in einem Kubernetes-Cluster von Administratoren oder dynamisch über Speicherklassen bereitgestellt wird.
Ein \ac{pvc} ist eine Anfrage zur Nutzung von \acs{pv}-Ressourcen in einem Kubernetes-Cluster.
Im Folgenden werden die Persistent-Volumes der MongoDB Kubernetes-Ressourcenobjekte erläutert.


\subsubsection{PersistentVolumeClaim}

Ähnlich wie ein Pod, Systemressourcen wie CPU und Hauptspeicher nutzt.
Benötigt ein \acs{pvc} Systemressourcen in Form von Festplattenspeicher.
Bei fehlenden \acs{pv} wird durch dynamische Provisionierung für die Anfrage ein \acs{pv} erstellt.

Die YAML-Datei für die MongoDB liegt in einem Unterverzeichnis mit den dazugehörigen Ressourcenobjekten.
Unter \textit{annotations} wird eine IF-Bedingung gestellt, die das Löschen nach der Nutzung des \acs{pvc} erlaubt, wenn der Wert wahr ist wird es nicht gelöscht.
Die \textit{annotations: }\textit{"helm.sh/resource-policy": keep} verhindert die Löschung einer Kubernetes-Ressource, wenn ein Helm-Chart deinstalliert wird \cite{helmtipsandtricks}.
Der \textit{spec} beschreibt die Speicherklassen des \acs{pvc}, dieser ist in dem Test-Cluster standardmäßig local-path.
Die \textit{accessModes} definieren die Zugriffsmodi des \acs{pv} wie Beispielsweise die Lese- und Schreibrechte mehrerer Clients.
Für die MongoDB wird der Modus \textit{ReadWriteOnce} verwendet.
Dieser erlaubt Lese- und Schreibrechte für Pods die sich auf derselben Node befinden.
\textit{Resources} definiert mit \textit{storage} die Menge an benötigten Speicherplatzes (vgl. Quellcode \ref{lst:pvc}).

\begin{lstlisting}[caption={pvc-claim.yaml },captionpos=b,label={lst:pvc},language=yaml]
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: {{ .Values.mongodbvolume.persistence.claimName }}
  annotations:
    {{- if .Values.skipuninstall }}
    "helm.sh/resource-policy": keep
    {{- end }}
spec:
  storageClassName: {{ .Values.mongodbvolume.persistence.storageClassName }}
  accessModes:
    {{- toYaml .Values.mongodbvolume.persistence.accessModes | nindent 4  }}
  resources:
    requests:
      storage: {{ .Values.mongodbvolume.persistence.storage }}
\end{lstlisting}


\subsubsection{PersistentVolume}

Der \acs{pvc} kann dann von dem MongoDB-Deployment referenziert werden 
und als Speicher in den Pod eingebunden werden. 
Das Standardverzeichnis für die Aufbewahrung von Daten in Mongodb ist \textit{/data/db} \cite{mongodbpath} (vgl. Quellcode \ref{lst:pv}).

\begin{lstlisting}[caption={Ausschnitt aus dem mongodb-deployment.yaml },captionpos=b,label={lst:pv},language=yaml]
  volumeMounts:
  - name: "mongo-data-dir"
    mountPath: "/data/db"
volumes:
- name: "mongo-data-dir"
  persistentVolumeClaim:
    claimName: "{{ .Values.mongodbvolume.persistence.claimName }}"
\end{lstlisting}
