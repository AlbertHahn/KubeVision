\chapter{Analyse}\label{Analyse}

Das vorherige Kapitel widmete sich dem Aufbau von Containern und deren Verwaltung.
Darauf aufbauend wurde auch die mögliche Realisierung von Anwendungen im Micorservice-Architektur-Stil besprochen. 
Dieses Kapitel widmet sich den Innovationsforschungen der Krones AG in Form eines Proof of Concepts.
Abschließend folgen die Resultate und möglichen neuen Anwendungsgebiete im Bereich Cloud-Technologie.


\section{Proof of Concept}\label{moderninfra}
Die Krones AG entwickelt neue Konzepte, um Produktionsanlagen standortübergreifend zu modernisieren. 
In einem davon wurde ein \ac{poc} mit dem Software-Unternehmen SUSE durchgeführt, um die Umsetzung neuer Cloud-Technologien zu evaluieren. 
Die folgenden Punkte behandeln die Kernthemen des \acs{poc} sowie dem Edge-Computing und Kubernetes.

\subsection{Edge-Computing}
Edge-Computing bezeichnet die dezentrale Verarbeitung von Daten in direkter Nähe der Datenquelle. 
Das verringert den Bedarf an lokale Rechenzentren und senkt die Latenzzeiten bei der Übertragung von Daten. 
Betrachtungsgenstand des PoC war die Virtualisierung der bereits vorhandenen Industrierechner der Firma B\&R, um diese als Virtual-Edge-Devices zu verwenden.
Auf den Virtual-Edge-Devices werden dann Operationen, wie Erfassen, Aggregieren und Aufbereiten von Daten, direkt an der Anlage ausgeführt. 
Die derzeitigen Anwendungen der Krones AG sind für das Betriebssystem Windows konzipiert und entwickelt worden.
Für das Edge-Szenario soll aber ein auf Linux basierendes Betriebssystem verwendet werden,
weshalb die Integration über Virualisierungsmöglichkeiten realisiert wird.

\subsubsection{Virtualisierung}
Das Unternehmen B\&R steht in Kooperation mit der Firma Real Time Systems, welches Technologien für die Virtualisierung von Echtzeit Betriebssystemen anbietet \cite{rtosandbundr}.
Dafür wird ein Hypervisor genutzt, um gleichzeitig unterschiedliche Echtzeitbetriebssysteme in Form von VMs auszuführen.
Dies ermöglicht auch die Zuteilung von Hardwareressourcen auf die laufenden VMs.
Ein Vorteil ist, dass keine zusätzliche Hardware benötigt wird.
Die Zuweisung für Hardwareschnittstellen, wie Ethernet und USB-Ports, ist durch diesen Ansatz auch möglich.
Virtuelle Netzwerke erlauben die Zuweisung von IPv4- und Mac-Adressen zu einzelnen Prozessorkernen, welche
eine direkte Kommunikation über Internetprotokolle, wie TCP/IP oder COBRA, ermöglichen.
Weiter kann jedes virtualisierte Betriebssystem Daten über eine gemeinsame Speicherpartition verwalten \cite{rtos}.

\subsubsection{Connected \ac{hmi}}
Als Betriebssystem nutzt die Produktionsanlage \textit{Windows 10 Embedded}, welches mit einem \acs{hmi} über eine Touch-Oberfläche Bedienbar ist.
Dieses ist für die zentrale Überwachung, Steuerung und Parametrisierung von Anlagenprozessen zuständig.
Und erlaubt die Einteilung von produktionsrelevanten Aufgaben, wie Wartungsarbeiten, Materialversorgung und Qualitätskontrollen \cite{hmi}.

\subsubsection{SUSE Linux Enterprise Micro}
Basierend auf der Idee von containerisierten Arbeistlasten und Microservices wurde das Betriebssystem \textit{SUSE Linux Enterprise Micro} entwickelt.
Das für Edge-Szenarien entwickelte Open-Source-Betriebssystem ist das zweite virtualisierte Betriebssystem, das auf dem Hypervisor laufen soll.
Es arbeitet mit \textit{transactional-updates}, welche Updates erst aktivieren, wenn das Betriebssystem neu gestartet wurde. 
Erfolgt das Update nicht, wird ein Rollback zum vorherigen Versionszustand durchgeführt. Dies ermöglicht wartungsfreie Zustände der Geräte.



\subsection{Kubernetes}
Auf der Grundlage des Edge-Computing war ein weiterer Schwerpunkt des PoC die Orchestrierung einer solchen Infrastruktur. 
Dabei sollen die einzelnen Virtual-Edge-Devices in Zukunft als Knotenpunkte für ein Kubernetes-Cluster dienen. 
Dafür wird die für Edge-Szenarien entworfene Kubernetes-Distribution k3s installiert.

Auf dieser sollen containerisierte Anwendungen ausgeliefert und bereitgestellt werden.
Anwendungen, wie Mircoservices, können innerhalb des Cluster über Endpunkte oder Kubernetes-Objekte kommunizieren.
Dadurch müssen Hostssysteme und die darauf laufende Software nicht mit deren Netzwerkadresse angesprochen werden,
und zustätzlich entfällt die Vorkonfiguration von Anwendungen durch die Architektur von Containern.


\subsubsection{Rancher}
Zur Orchestrierung der Kubernetes-Cluster-Instanzen wurde die Orchestrierungsplattform Rancher näher untersucht. 
Diese ermöglicht die zentrale Verwaltung mehrerer Kubernetes-Cluster in Produktionsumgegebungen.
Weiterhin wurde die Bedienbarkeit der Benutzeroberfläche untersucht und es wurden Tests durchgeführt, bei welchen
containerisierte Arbeitslasten verwaltet und Kubernetes-Objekte erstellt und miteinander verbunden wurden.
Ein weiterer Vorteil ist die Auslieferung von Kubernetes-Anwendungen durch den \textit{Apps \& Marketplace von Rancher}. 

\subsubsection{Helm}
Helm ist ein Kubernetes-Package-Manager und ermöglicht die Erstellung, die Installation und das Updaten von Kubernetes-Anwendungen.
Wichtige Konzepte von Helm sind Charts, die eine Ansammlung von Informationen zur Erstellung von Anwendungen als Kubernetes-Instanz sind. 
Configs beinhalten die Informationen der Konfiguration von Charts und erstellen oder verpacken diese.
Ein Release bezeichnet eine laufende Instanz eines Charts in Kombination mit einer spezifischen Config \cite{helm}.

Helm ist eine ausführbare Datei, die aus einem Kommandozeilentool besteht, dem Helm-Client.
Dieser erlaubt die lokale Entwicklung von Charts und dem Verwalten von Repositoriy und unterschiedlichen Versionen.
Weiterhin dient der Client als Schnittstelle zur Helm-Library, welche Operationen mit dem Kubernetes-API-Server ermöglichen.
Dadurch können Charts und Konfigurationen als ein Release gebildet
und Charts in einem Kubernetes-Cluster installiert, deinstalliert und aktualisiert werden.
Die Konfigurationsdateien von Helm werden in der Regel, wie auch bei Kubernetes-Ressourcenobjekten üblich, in YAML geschrieben \cite{helm}.

Der Anwendungsfall in Bezug auf die Krones AG ist die kundenindividuelle Konfiguration von Kubernetes-Anwendungen, die in einem Kunden Repository gespeichert werden.
Die Kundenkonfigurationen stehen dabei als Helm-Charts verfügbar und ermöglichen eine vereinfachte Auslieferung und Bereitstellung durch Helm-Repositories.
Der Rancher \textit{Apps \& Marketplace} ist hierbei eine Möglichkeit, Helm-Charts über eine Benutzeroberfläche zu konfigurieren und installieren.

\newpage

\section{Resultate}
Die Bestrebungen des PoC zeigten neue Anwendungsgebiete für die weitere Untersuchung von relevanten Themen im Zusammenhang mit Kubernetes.
Während der Umsetzungsphase zeigten folgende Themen Potenzial.

\subsubsection{Hybrid-Cloud}
Die zukünftige Kubernetes-Infrastruktur kann die Integration von On-Premise und Cloud-Ressourcen in einer Softwareumgegbung ermöglichen.
Kunden können sensible Daten in ihrer eigenen privaten Cloud oder einem lokalen Rechenzentrum speichern und gleichzeitig die
Vorteile von den erhöhten Rechenressourcen einer verwalteten Public-Cloud nutzen.
Kubernetes verfügt über Funktionen, die eine Aufteilung der Arbeitslasten in spezifische Cloud-Umgebungen ermöglicht.

Die Modularität des Kubernetes-Cluster ermöglicht die Steigerung der Gesamtleistung durch Cloud-Ressourcen oder der Integration von neuer Hardware vor Ort.
Dies erfordert auch die gegebene Modularität der Softwarekomponenten auf dem Kubernetes-Cluster.

\subsubsection{Microservices}
Wie in Abschnitt \ref{Microservice} behandelt, werden die Kernfunktionalitäten der zu entwickelnden Anwendung in einzelne Dienste aufgeteilt.
Anwendungen können in Zukunft auf Virtual-Edge-Devices oder in der Cloud bereitgestellt werden.
Die modulare Entwicklung von Anwendungen ermöglicht es, die geringe Rechenleistung von Virtual-Edge-Devices zu kombinieren.
Der zukünftige Kunde kann bei diesem Ansatz eine Auswahl an Diensten treffen, die er für seine Anlage benötigt.

Ein weiterer Vorteil ist, dass eine modulare Architektur auf Containern die Auslieferung und Bereistellung erleichtert.
Der Package-Manager Helm kann dabei die Vorkonfiguration der einzelnen Dienste gewährleisten.


\subsubsection{Künstliche Intelligenz}
Das Kubernetes-Cluster bietet eine Infrastruktur für Anwendungen mit Bezug zu Themen aus der künstlichen Intelligenz.
Denn die Verwaltung von Prozessen im Bereich Machine Learning umfasst Lebenszyklen mit mehreren Phasen.
Diese beinhalten beispielsweise die Datenverarbeitung, Merkmalsextraktion, Modelltraining und Anwendungsbereitstellung.
Die Ausführung der Phasen erzeugt Artefakte in Form von Datensätzen, Modellen und Anwendungskonfigurationen.
Plattformen mit dieser Arbeitslast benötigen eine erhebliche Menge an Systemressourcen und ein stabiles Netzwerk \cite{mlops}.
Die Verarbeitung der Arbeitslast kann von Cloud-Ressourcen oder On-Premise Hardware mit \acp{gpu} übernommen werden.
